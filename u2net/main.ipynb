{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D U^2-Net: A 3D Universal U-Net for Multi-Domain Medical Image Segmentation:\n",
    "Test things here\n",
    "https://theaisummer.com/unet-architectures/\n",
    "\n",
    "https://github.com/patrick-kidger/torchtyping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "The Medical Segmentation Decathalon data is assumed to be provided in a particular organizational structure with respect to this folder. For the sake of this exercise, I only will be using three of the provided datasets. If you have these files in the correct folder, you should receive the same (or similar) results.\n",
    "\n",
    "```\n",
    "../data/U2Net/\n",
    "        Task02_Heart/\n",
    "            imagesTr/\n",
    "                la_003.nii.gz\n",
    "                ...\n",
    "                la_030.nii.gz\n",
    "            imagesTs/\n",
    "                la_001.nii.gz\n",
    "                ...\n",
    "                la_028.nii.gz\n",
    "            labelsTr/\n",
    "                la_003.nii.gz\n",
    "                ...\n",
    "                la_030.nii.gz\n",
    "        \n",
    "        Task04_Hippocampus/\n",
    "            imagesTr/\n",
    "                hippocampus_001.nii.gz\n",
    "                ...\n",
    "                hippocampus_394.nii.gz\n",
    "            imagesTs/\n",
    "                hippocampus_002.nii.gz\n",
    "                ...\n",
    "                hippocampus_392.nii.gz\n",
    "            labelsTr/\n",
    "                hippocampus_001.nii.gz\n",
    "                ...\n",
    "                hippocampus_394.nii.gz\n",
    "\n",
    "        Task05_Prostate/\n",
    "            imagesTr/\n",
    "                prostate_00.nii.gz\n",
    "                ...\n",
    "                prostate_47.nii.gz\n",
    "            imagesTs/\n",
    "                prostate_03.nii.gz\n",
    "                ...\n",
    "                prostate_45.nii.gz\n",
    "            labelsTr/\n",
    "                prostate_00.nii.gz\n",
    "                ...\n",
    "                prostate_47.nii.gz\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "sys.path.append('..') # Stupid thing Python makes you do to import from a sibling directory\n",
    "from gen_utils.ImgTools import ImgAug # Custom class for image generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image creation class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Iterable, Tuple, Any, Union, Generator, Dict\n",
    "from pathlib import Path\n",
    "\n",
    "class UData(ImgAug):\n",
    "    '''Class for data management for U^2-Net training and testing\n",
    "    \n",
    "    Parameters\n",
    "    -------\n",
    "    - path pairs for the folders containing the raw images and the labels\n",
    "        [[/img_1, /label_1],[/img_2, /label_2]]\n",
    "    - output directory for generated images (after patches/augmentation is applied)\n",
    "\n",
    "    '''\n",
    "    def __init__(self, img_dir: str, label_dir: str, img_out_dir: str, label_out_dir: str, prefix: str='', suffix: str='') -> None:\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.img_out_dir = img_out_dir\n",
    "        self.label_out_dir = label_out_dir\n",
    "        self.in_img_files, self.in_img_paths = self.get_files(img_dir, prefix, suffix)\n",
    "        self.in_label_files, self.in_label_paths = self.get_files(label_dir, prefix, suffix)\n",
    "        self.out_img_files = []\n",
    "        self.out_label_files = []\n",
    "\n",
    "\n",
    "    def get_files(self, file_dir:str, prefix:str, suffix:str) -> Tuple[List[str],List[str]]:\n",
    "        files = []\n",
    "        paths = []\n",
    "        # If they have provided a list of directories (in the case of DICOM or scattered data)\n",
    "        if isinstance(file_dir, list):\n",
    "            for inp_dir in file_dir:\n",
    "                for fil in os.listdir(inp_dir):\n",
    "                    if fil.startswith(prefix) and fil.endswith(suffix):\n",
    "                        paths.append(inp_dir + fil)\n",
    "                        files.append(fil)\n",
    "\n",
    "                    if not files:\n",
    "                        raise FileNotFoundError('No applicable files found in input directory')\n",
    "        else:\n",
    "            for fil in os.listdir(file_dir):\n",
    "                if fil.startswith(prefix) and fil.endswith(suffix):\n",
    "                    paths.append(file_dir + fil)\n",
    "                    files.append(fil)\n",
    "\n",
    "                if not files:\n",
    "                    raise FileNotFoundError('No applicable files found in input directory')\n",
    "\n",
    "        return files, paths\n",
    "\n",
    "    def match_files(self, img_dir: str, label_dir: str, update=False, paths=True) -> Tuple[List[Path], List[Path]]:\n",
    "        # Get the files that have been generated in the output directory\n",
    "        # If update is false, then just return a list of matched names, if true then\n",
    "        # change the class variable values accordingly.\n",
    "        hr_files = os.listdir(img_dir)\n",
    "        lr_files = os.listdir(label_dir)\n",
    "\n",
    "        # Get a set of all the files with agreement before the metadata\n",
    "        if len(hr_files) > len(lr_files):\n",
    "            matches = list(set(hr_files)-(set(hr_files)-set(lr_files)))\n",
    "        else:\n",
    "            matches = list(set(lr_files)-(set(lr_files)-set(hr_files)))\n",
    "\n",
    "        if update:\n",
    "            # If you want to save these matched files as class variables\n",
    "            self.out_img_files = [Path(img_dir + _) for _ in matches]\n",
    "            self.out_label_files = [Path(label_dir + _) for _ in matches]\n",
    "            print('Image and Lable file locations updated')\n",
    "        \n",
    "        if paths:\n",
    "            return [Path(img_dir + _) for _ in matches], [Path(label_dir + _ ) for _ in matches]\n",
    "        \n",
    "        return [], [] #lazy to make typing work out\n",
    "\n",
    "    def load_image_pair(self, im_id: Union[int, str] ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        # im_id can either be the index value or the name of the file\n",
    "        \n",
    "        if self.out_label_files:\n",
    "            if isinstance(im_id, int):\n",
    "                img_file = self.out_img_files[im_id]\n",
    "                label_file = self.out_label_files[im_id]\n",
    "            elif isinstance(im_id, str):\n",
    "                _ = self.out_img_files.index(Path(im_id))\n",
    "                img_file = self.out_img_files[_]\n",
    "                label_file = self.out_label_files[_]\n",
    "            else:\n",
    "                TypeError(\"Invalid image identifier, please input a string to integer\")\n",
    "\n",
    "            img = self.load_image(img_file)\n",
    "            lab = self.load_image(label_file)\n",
    "\n",
    "            return img, lab\n",
    "        else:\n",
    "            raise ValueError(\"No paths for processed image/label files are stored in this class\")\n",
    "\n",
    "\n",
    "    def run(self, clear=False, save=False, contain_lab: bool=False, verbose=False) -> None:\n",
    "        \n",
    "\n",
    "        if clear:\n",
    "            print('Clearing existing output directories')\n",
    "            shutil.rmtree(self.img_out_dir, ignore_errors=True)\n",
    "            shutil.rmtree(self.label_out_dir, ignore_errors=True)\n",
    "            \n",
    "\n",
    "        os.makedirs(self.img_out_dir, exist_ok=True)\n",
    "        os.makedirs(self.label_out_dir, exist_ok=True)\n",
    "        \n",
    "        fnames_h = []\n",
    "        fnames_l = []\n",
    "\n",
    "        # match in_image_files and in_label_files\n",
    "\n",
    "        #TODO: Come up with good way for match_files to handle multiple input directories\n",
    "        self.in_img_paths, self.in_label_paths = self.match_files(self.img_dir, self.label_dir, update=False, paths=True)\n",
    "\n",
    "        aug_params = {\"translation\":[10,10,10]}\n",
    "        patch = [50, 50, 1]\n",
    "        step = [20, 20, 2]\n",
    "\n",
    "        rand_params_gen = self.gen_random_aug(aug_params)\n",
    "\n",
    "        # for each image, label in in_img_files:\n",
    "        out_img_files = []\n",
    "        out_label_files = []\n",
    "\n",
    "        for im_p, lab_p in zip(self.in_img_paths, self.in_label_paths):\n",
    "\n",
    "            # generate a random parameter set\n",
    "            rand_params = next(rand_params_gen)\n",
    "\n",
    "            # Load images\n",
    "            im = self.load_image(im_p)\n",
    "            lab = self.load_image(lab_p)\n",
    "\n",
    "            # apply image augmentations to pairs of images\n",
    "            im, im_suf = self.array_translate(im, rand_params['translation'])\n",
    "            lab, lab_suf = self.array_translate(lab, rand_params['translation'])\n",
    "\n",
    "            # save as patches of size [x,y,z]\n",
    "            \n",
    "            if contain_lab: #Whether to only take patches which contain the label of interest\n",
    "\n",
    "                fname = lab_p.stem\n",
    "                _, b, not_lab = self.img2patches(lab, patch[:], step[:], min_nonzero= 0.3, fname=fname+lab_suf, save=[self.label_out_dir,'.nii'], verbose = False)\n",
    "                out_label_files.extend(b)\n",
    "\n",
    "                fname = im_p.stem\n",
    "                _, a, not_img = self.img2patches(im, patch[:], step[:], fname=fname+im_suf, slice_select=not_lab, save=[self.img_out_dir,'.nii'], verbose = False)\n",
    "                out_img_files.extend(a)\n",
    "\n",
    "            else:\n",
    "                fname = im_p.stem\n",
    "                _, a, not_img = self.img2patches(im, patch[:], step[:], fname=fname+im_suf, save=[self.img_out_dir,'.nii'], verbose = False)\n",
    "                out_img_files.extend(a)\n",
    "\n",
    "                fname = lab_p.stem\n",
    "                _, b, not_lab = self.img2patches(lab, patch[:], step[:], fname=fname+lab_suf, slice_select=not_img, save=[self.label_out_dir,'.nii'], verbose = False)\n",
    "                out_label_files.extend(b)\n",
    "            \n",
    "\n",
    "\n",
    "        # update file locations for use with load_image_pair\n",
    "        self.out_img_files = out_img_files\n",
    "        self.out_label_files = out_label_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U^2-net blocks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int) -> None:\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class InConv(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int) -> None:\n",
    "        super(InConv, self).__init__()\n",
    "        self.conv = DoubleConv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super(Down, self).__init__()\n",
    "        self.mpconv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_ch, out_ch)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.mpconv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, bilinear: bool=True) -> None:\n",
    "        super(Up, self).__init__()\n",
    "\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch // 2, in_ch // 2, 2, stride=2)\n",
    "\n",
    "        self.conv = DoubleConv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:\n",
    "        x1 = self.up(x1)\n",
    "\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, (diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2))\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, in_channels: int, classes: int) -> None:\n",
    "        super(Unet, self).__init__()\n",
    "        self.n_channels = in_channels\n",
    "        self.n_classes =  classes\n",
    "\n",
    "        self.inc = InConv(in_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 512)\n",
    "        self.up1 = Up(1024, 256)\n",
    "        self.up2 = Up(512, 128)\n",
    "        self.up3 = Up(256, 64)\n",
    "        self.up4 = Up(128, 64)\n",
    "        self.outc = OutConv(64, classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.outc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed U^2-Net:\n",
    "Adapter types:\n",
    "\n",
    "`'series_adapter'` = Series Residual Adapter\n",
    "\n",
    "`'parallel_adapter'` = Parallel Residual Adapter\n",
    "\n",
    "`'separable_adapter'` = Residual Adapter which incorporates depthwise seperable convolution (what the paper is proposing)\n",
    "\n",
    "Model types:\n",
    "\n",
    "`'universal'` = Model proposed in manuscript that maintains pointwise convoltion parameters across domains while having domain-specific depthwise convolution\n",
    "\n",
    "`'independent'` = Generic U-Net model trained on single image domain\n",
    "\n",
    "`'shared'` = Generic U-Net trained on combinaiton of all image domains(?)\n",
    "\n",
    "- All model types seem to use some sort of [residual adapter](https://www.researchgate.net/figure/Series-vs-parallel-residual-adapters-a-typical-module-of-a-residual-network-inclusive_fig1_324055530) inspired by this [repository](https://github.com/srebuffi/residual_adapters/)\n",
    "\n",
    "\n",
    "Original code key:\n",
    "`nb_tasks` = The number of unique tasks that you are using the model for. So if there are two different image sets you want different models for, then `nb_tasks` = 2\n",
    "\n",
    "`config.module` = the type of adapter you wish to use\n",
    "\n",
    "`config.trainMode` = the model type you wish to use\n",
    "\n",
    "`config.task_idx` = the id number associated with a particular task, this allows for selecting the weights by index for a particular task. So in `q = nn.ModuleList(...)` if you want to use the weights associated with task `i` you use `q[i](input)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U2-Net steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "def num_pool2stride_size(num_pool_per_axis):\n",
    "    '''\n",
    "    Calculate the stride size for the max pooling which occurs, If the num_pool_per_axis[i] for axis 2 is less than the maximum, then\n",
    "    instead of a 2x2x2 pooling it will be a 2x1x2 pooling. With a step size of 1 along the axis, the pooling will not downsample it\n",
    "\n",
    "    \"The number of down-sampling operations per axis is set until the feature map size of the deepest layer reaches as small as 8\"\n",
    "    '''\n",
    "    max_num = max(num_pool_per_axis)\n",
    "    stride_size_per_pool = list()\n",
    "    for i in range(max_num):\n",
    "        unit = [1,2]\n",
    "        stride_size_per_pool.append([unit[i<num_pool_per_axis[0]], unit[i<num_pool_per_axis[1]], unit[i<num_pool_per_axis[2]]])\n",
    "    return stride_size_per_pool\n",
    "\n",
    "\n",
    "class U2Net3D(nn.Module):\n",
    "    def __init__(self, inChans_list: 'List[int]', num_pool_per_axis: 'List[int]', base_outChans: int=16, num_class_list: 'List[int]'=[4], \n",
    "    depth: int=5, trainMode: str='universal', module: str='seperable_adapter') -> None:\n",
    "        '''\n",
    "        Parameters:\n",
    "        inChans_list: list of input channels for the differnet image types you will be using\n",
    "        num_pool_per_axis: the number of times you wish for this axis to be downsampled by 2x2 max pooling in the endocer portion.\n",
    "                If you have a U-Net with depth 5 but only want to downsample the x-axis 3 times, then you would put [3,5,5]\n",
    "        base_out_Chans: number of output channels for the input image before it goes through the U^2-Net\n",
    "        num_class_list: \n",
    "        depth: the depth of the U-Net\n",
    "        trainMode: The U-Net model type being used\n",
    "        '''\n",
    "\n",
    "        super(U2Net3D, self).__init__()\n",
    "        self.depth = depth\n",
    "        #trainMode = the model type being used\n",
    "        self.trainMode = trainMode\n",
    "\n",
    "        nb_tasks = len(num_class_list)\n",
    "\n",
    "        self.depth = max(num_pool_per_axis) + 1 # config.num_pool_per_axis firstly defined in train_xxxx.py or main.py\n",
    "        stride_sizes = num_pool2stride_size(num_pool_per_axis)\n",
    "\n",
    "        # Create a module list of the InputTransitions for each input data type\n",
    "        # In other words, if you are running this model on a set of images with 2 channels and a set of\n",
    "        # images with 3 channels, then self.in_tr_list = nn.ModuleList([[2x InputTran],[3x InputTran]])\n",
    "        \n",
    "        # TODO: In this implementaiton, they are applying a 3x3x3 instead of a 1x1x1 like they say in the paper \n",
    "        self.in_tr_list = nn.ModuleList(\n",
    "            [InputTransition(inChans_list[j], base_outChans) for j in range(nb_tasks)]\n",
    "        )\n",
    "\n",
    "        outChans_list = list()\n",
    "        self.down_blocks = nn.ModuleList() #register modules from regular python list.\n",
    "        self.down_samps = nn.ModuleList()\n",
    "        self.down_pads = list() # used to pad as padding='same' in tensorflow\n",
    "\n",
    "        inChans = base_outChans\n",
    "\n",
    "        # Create each level of the encoder and decoder\n",
    "        for i in range(depth):\n",
    "            outChans = base_outChans * (2**i)\n",
    "            outChans_list.append(outChans)\n",
    "\n",
    "            # Add another encoder level to the down_block module list\n",
    "            self.down_blocks.append(DownBlock(nb_tasks, inChans, outChans, trainMode, module, kernel_size=3, stride=1, padding=1))\n",
    "\n",
    "            if i != self.depth-1:\n",
    "                # stride for each axis could be 1 or 2, depending on tasks. # to apply padding='SAME' as tensorflow, cal and save pad num to manually pad in forward().\n",
    "                pads = list() # 6 elements for one 3-D volume. originized for last dim backward to first dim, e.g. w,w,h,h,d,d # required for F.pad.\n",
    "                # pad 1 to the right end if s=2 else pad 1 to both ends (s=1). \n",
    "                for j in stride_sizes[i][::-1]:\n",
    "                    if j == 2:\n",
    "                        pads.extend([0,1])\n",
    "                    elif j == 1:\n",
    "                        pads.extend([1,1])\n",
    "                self.down_pads.append(pads) \n",
    "                self.down_samps.append(DownSample(nb_tasks, outChans, outChans*2, trainMode, module, kernel_size=3, stride=tuple(stride_sizes[i]), padding=0))\n",
    "                inChans = outChans*2\n",
    "            else:\n",
    "                inChans = outChans\n",
    "\n",
    "        ## END OF ENCODING\n",
    "        ## BEGINNING OF DECODING\n",
    "        print('end')\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x:torch.Tensor, task_idx: int=0) -> torch.Tensor:\n",
    "        self.task_idx = task_idx #What is the index associated with the dataset you are currently using\n",
    "        deep_supervision = None\n",
    "\n",
    "        out = self.in_tr_list[task_idx](x)\n",
    "\n",
    "        down_list = list()\n",
    "        for i in range(self.depth):\n",
    "            out = self.down_blocks[i](out)\n",
    "            # down_list.append(out)\n",
    "            if i != self.depth-1:\n",
    "                down_list.append(out) # will not store the deepest, so as to save memory\n",
    "                # manually padding='SAME' as tensorflow before down sampling\n",
    "                out = F.pad(out,tuple(self.down_pads[i]), mode=\"constant\", value=0)\n",
    "                out = self.down_samps[i](out)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    # TODO: DownSampling is not 2x2 MaxPooling, but simply a convolution with a \n",
    "    def __init__(self, nb_tasks, inChans, outChans, trainMode, module, kernel_size=3, stride=1, padding=1) -> None:\n",
    "        super(DownSample, self).__init__()\n",
    "        self.op1 = conv_unit(nb_tasks, inChans, outChans, trainMode, module, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.act1 = norm_act(outChans, meth=\"act\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.op1(x)\n",
    "        out = self.act1(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, nb_tasks, inChans, outChans, trainMode, module: str='seperable_adapter', residual: bool=True, kernel_size=3, stride=1, padding=1) -> None:\n",
    "        super(DownBlock, self).__init__()\n",
    "        self.module = module\n",
    "        self.residual = residual\n",
    "        self.op1 = conv_unit(nb_tasks, inChans, outChans, trainMode, module, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.act1 = norm_act(outChans, meth=\"act\")\n",
    "        self.op2 = conv_unit(nb_tasks, outChans, outChans, trainMode, module, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.act2 = norm_act(outChans, meth=\"act\")\n",
    "\n",
    "    def forward(self, x)-> torch.Tensor:\n",
    "\n",
    "        # TODO: These if/else statements are redundant...\n",
    "\n",
    "        if self.module == 'parallel_adapter' or self.module == 'separable_adapter':\n",
    "            out, share_map, para_map = self.op1(x)\n",
    "        else:\n",
    "            out = self.op1(x)\n",
    "        out = self.act1(out)\n",
    "        if self.module == 'parallel_adapter' or self.module == 'separable_adapter':\n",
    "            out, share_map, para_map = self.op2(out)\n",
    "        else:\n",
    "            out = self.op2(out)\n",
    "        if self.residual: # same to ResNet\n",
    "            out = self.act2(x + out)\n",
    "        else:\n",
    "            out = self.act2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class conv_unit(nn.Module):\n",
    "    '''\n",
    "    Conv3D with the addition of adapter, if applicable\n",
    "    '''\n",
    "    def __init__(self, nb_tasks, inChans, outChans, trainMode: str, module, kernel_size: int=3, stride: int=1, padding: int=1) -> None:\n",
    "        super(conv_unit, self).__init__()\n",
    "        self.stride = stride\n",
    "\n",
    "        if self.stride !=1:\n",
    "            self.conv = nn.Conv3d(inChans, outChans, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        elif self.stride == 1:\n",
    "            if trainMode != 'universal': # If not universal, then use generic U-Net setup\n",
    "                self.conv = nn.Conv3d(inChans, outChans, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "            \n",
    "            elif module in ['series_adapter', 'parallel_adapter']:\n",
    "                self.conv = nn.Conv3d(inChans, outChans, kernel_size=kernel_size, stride=stride, padding=padding) # padding != 0 for stride != 2 if doing padding=SAME.\n",
    "                if module == 'series_adapter':\n",
    "                    self.adapOps = nn.ModuleList([conv1x1(outChans) for i in range(nb_tasks)]) # based on https://github.com/srebuffi/residual_adapters/\n",
    "                elif module == 'parallel_adapter':\n",
    "                    self.adapOps = nn.ModuleList([conv1x1(inChans, outChans) for i in range(nb_tasks)]) \n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            elif module == 'seperable_adapter':\n",
    "                self.adapOps = nn.ModuleList([dwise(inChans) for i in range(nb_tasks)])\n",
    "                self.pwise = pwise(inChans, outChans)\n",
    "\n",
    "        self.op = nn.ModuleList([norm_act(outChans, meth='norm') for i in range(nb_tasks)])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        task_idx = self.task_idx\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        if self.stride != 1:\n",
    "            out = self.conv(x)\n",
    "            out = self.op[task_idx](out)\n",
    "            return out\n",
    "        elif self.stride == 1:\n",
    "            if self.trainMode != 'universal': # independent, shared\n",
    "                out = self.conv(x)\n",
    "                out = self.op[task_idx](out)\n",
    "            else: #universal\n",
    "                if self.module in ['series_adapter', 'parallel_adapter']:\n",
    "                    out = self.conv(x)\n",
    "                    if self.module == 'series_adapter':\n",
    "                        out = self.adapOps[task_idx](out)\n",
    "                    elif self.module == 'parallel_adapter':\n",
    "                        share_map = out\n",
    "                        para_map = self.adapOps[task_idx](x)\n",
    "                        out = out + para_map\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                    out = self.op[task_idx](out)\n",
    "                    if self.module == 'parallel_adapter':\n",
    "                        return out, share_map, para_map # for visualization of feature maps\n",
    "                    else:\n",
    "                        return out\n",
    "                elif self.module == 'separable_adapter':\n",
    "                    out = self.adapOps[task_idx](x)\n",
    "                    para_map = out\n",
    "                    out = self.pwise(out)\n",
    "                    share_map = out\n",
    "                    out = self.op[task_idx](out)\n",
    "                    return out, share_map, para_map\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "\n",
    "class dwise(nn.Module):\n",
    "    '''Depthwise convolution+normalization'''\n",
    "    def __init__(self, inChans, kernel_size: int=3, stride: int=1, padding: int=1) -> None:\n",
    "        super(dwise, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(inChans, inChans, kernel_size=kernel_size, stride=stride, padding=padding, groups=inChans) #By setting groups=inChans, the kernels are only applied to single channels\n",
    "        self.op1 = norm_act(inChans,meth='both')\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.conv1(x)\n",
    "        out = self.op1(out)\n",
    "        return out\n",
    "\n",
    "class pwise(nn.Module):\n",
    "    '''Pointwise convolution?\n",
    "    '''\n",
    "    def __init__(self, inChans:int, outChans:int, kernel_size: int=1, stride: int=1, padding: int=0) -> None:\n",
    "        super(pwise, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(inChans, outChans, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        out = self.conv1(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class InputTransition(nn.Module):\n",
    "    '''\n",
    "    task specific\n",
    "    '''\n",
    "    def __init__(self, inChans, base_outChans):\n",
    "        super(InputTransition, self).__init__()\n",
    "        self.op1 = nn.Sequential(\n",
    "            nn.Conv3d(inChans, base_outChans, kernel_size=3, stride=1, padding=1),\n",
    "            norm_act(base_outChans)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.op1(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class conv1x1(nn.Module):\n",
    "    def __init__(self, inChans, outChans=1, stride=1, padding=0):\n",
    "        super(conv1x1, self).__init__()\n",
    "        if self.module == 'series_adapter':\n",
    "            self.op1 = nn.Sequential(\n",
    "                norm_act(inChans,meth='norm'),\n",
    "                nn.Conv3d(inChans, inChans, kernel_size=1, stride=1)\n",
    "                )\n",
    "        elif self.module == 'parallel_adapter':\n",
    "            self.op1 = nn.Conv3d(inChans, outChans, kernel_size=1, stride=stride, padding=padding)\n",
    "        else: #seperable adapter\n",
    "            self.op1 = nn.Conv3d(inChans, inChans, kernel_size=1, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.op1(x)\n",
    "        if self.module == 'series_adapter':\n",
    "            out += x\n",
    "        return out\n",
    "\n",
    "def norm_act(nchan, meth='both'):\n",
    "    '''The normalization activation function, or what normalization you are applying to the\n",
    "    data. Either a InstanceNorm3D for meth='norm', a Leaky ReLU for meth = 'act', \n",
    "    or an LeakyReLU(InstanceNorm3D(x)) for meth='both'\n",
    "     '''\n",
    "    norm = nn.InstanceNorm3d(nchan, affine=True) #Calculates the mean and standard deviation across each individual channel for a single example and uses that to normalize values\n",
    "    # act = nn.ReLU() # activation\n",
    "    act = nn.LeakyReLU(negative_slope=1e-2)\n",
    "    if meth=='norm':\n",
    "        return norm\n",
    "    elif meth=='act':\n",
    "        return act\n",
    "    else:\n",
    "        return nn.Sequential(norm, act)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "qq =  U2Net3D(inChans_list = [2], num_pool_per_axis = [5,5,5], num_class_list=[4], depth=5, trainMode = 'universal') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoader\n",
    "Data loader need to also provide some sort of identification based on the dataset being used if `universal` is the `self.trainMode` value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit ('3.8.7')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "539b544e2c3fdc58492248d082a132f5e0b4fea63e914fb274c32873997cf2f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
