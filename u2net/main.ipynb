{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D U^2-Net: A 3D Universal U-Net for Multi-Domain Medical Image Segmentation:\n",
    "Test things here\n",
    "https://theaisummer.com/unet-architectures/\n",
    "\n",
    "https://github.com/patrick-kidger/torchtyping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "The Medical Segmentation Decathalon data is assumed to be provided in a particular organizational structure with respect to this folder. For the sake of this exercise, I only will be using three of the provided datasets. If you have these files in the correct folder, you should receive the same (or similar) results.\n",
    "\n",
    "```\n",
    "../data/U2Net/\n",
    "        Task02_Heart/\n",
    "            imagesTr/\n",
    "                la_003.nii.gz\n",
    "                ...\n",
    "                la_030.nii.gz\n",
    "            imagesTs/\n",
    "                la_001.nii.gz\n",
    "                ...\n",
    "                la_028.nii.gz\n",
    "            labelsTr/\n",
    "                la_003.nii.gz\n",
    "                ...\n",
    "                la_030.nii.gz\n",
    "        \n",
    "        Task04_Hippocampus/\n",
    "            imagesTr/\n",
    "                hippocampus_001.nii.gz\n",
    "                ...\n",
    "                hippocampus_394.nii.gz\n",
    "            imagesTs/\n",
    "                hippocampus_002.nii.gz\n",
    "                ...\n",
    "                hippocampus_392.nii.gz\n",
    "            labelsTr/\n",
    "                hippocampus_001.nii.gz\n",
    "                ...\n",
    "                hippocampus_394.nii.gz\n",
    "\n",
    "        Task05_Prostate/\n",
    "            imagesTr/\n",
    "                prostate_00.nii.gz\n",
    "                ...\n",
    "                prostate_47.nii.gz\n",
    "            imagesTs/\n",
    "                prostate_03.nii.gz\n",
    "                ...\n",
    "                prostate_45.nii.gz\n",
    "            labelsTr/\n",
    "                prostate_00.nii.gz\n",
    "                ...\n",
    "                prostate_47.nii.gz\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "sys.path.append('..') # Stupid thing Python makes you do to import from a sibling directory\n",
    "from gen_utils.ImgTools import ImgAug # Custom class for image generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image creation class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Iterable, Tuple, Any, Union, Generator, Dict\n",
    "from pathlib import Path\n",
    "\n",
    "class UData(ImgAug):\n",
    "    '''Class for data management for U^2-Net training and testing\n",
    "    \n",
    "    Parameters\n",
    "    -------\n",
    "    - path pairs for the folders containing the raw images and the labels\n",
    "        [[/img_1, /label_1],[/img_2, /label_2]]\n",
    "    - output directory for generated images (after patches/augmentation is applied)\n",
    "\n",
    "    '''\n",
    "    def __init__(self, img_dir: str, label_dir: str, img_out_dir: str, label_out_dir: str, prefix: str='', suffix: str='') -> None:\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.img_out_dir = img_out_dir\n",
    "        self.label_out_dir = label_out_dir\n",
    "        self.in_img_files, self.in_img_paths = self.get_files(img_dir, prefix, suffix)\n",
    "        self.in_label_files, self.in_label_paths = self.get_files(label_dir, prefix, suffix)\n",
    "        self.out_img_files = []\n",
    "        self.out_label_files = []\n",
    "\n",
    "\n",
    "    def get_files(self, file_dir:str, prefix:str, suffix:str) -> Tuple[List[str],List[str]]:\n",
    "        files = []\n",
    "        paths = []\n",
    "        # If they have provided a list of directories (in the case of DICOM or scattered data)\n",
    "        if isinstance(file_dir, list):\n",
    "            for inp_dir in file_dir:\n",
    "                for fil in os.listdir(inp_dir):\n",
    "                    if fil.startswith(prefix) and fil.endswith(suffix):\n",
    "                        paths.append(inp_dir + fil)\n",
    "                        files.append(fil)\n",
    "\n",
    "                    if not files:\n",
    "                        raise FileNotFoundError('No applicable files found in input directory')\n",
    "        else:\n",
    "            for fil in os.listdir(file_dir):\n",
    "                if fil.startswith(prefix) and fil.endswith(suffix):\n",
    "                    paths.append(file_dir + fil)\n",
    "                    files.append(fil)\n",
    "\n",
    "                if not files:\n",
    "                    raise FileNotFoundError('No applicable files found in input directory')\n",
    "\n",
    "        return files, paths\n",
    "\n",
    "    def match_files(self, img_dir: str, label_dir: str, update=False, paths=True) -> Tuple[List[Path], List[Path]]:\n",
    "        # Get the files that have been generated in the output directory\n",
    "        # If update is false, then just return a list of matched names, if true then\n",
    "        # change the class variable values accordingly.\n",
    "        hr_files = os.listdir(img_dir)\n",
    "        lr_files = os.listdir(label_dir)\n",
    "\n",
    "        # Get a set of all the files with agreement before the metadata\n",
    "        if len(hr_files) > len(lr_files):\n",
    "            matches = list(set(hr_files)-(set(hr_files)-set(lr_files)))\n",
    "        else:\n",
    "            matches = list(set(lr_files)-(set(lr_files)-set(hr_files)))\n",
    "\n",
    "        if update:\n",
    "            # If you want to save these matched files as class variables\n",
    "            self.out_img_files = [Path(img_dir + _) for _ in matches]\n",
    "            self.out_label_files = [Path(label_dir + _) for _ in matches]\n",
    "            print('Image and Lable file locations updated')\n",
    "        \n",
    "        if paths:\n",
    "            return [Path(img_dir + _) for _ in matches], [Path(label_dir + _ ) for _ in matches]\n",
    "        \n",
    "        return [], [] #lazy to make typing work out\n",
    "\n",
    "    def load_image_pair(self, im_id: Union[int, str] ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        # im_id can either be the index value or the name of the file\n",
    "        \n",
    "        if self.out_label_files:\n",
    "            if isinstance(im_id, int):\n",
    "                img_file = self.out_img_files[im_id]\n",
    "                label_file = self.out_label_files[im_id]\n",
    "            elif isinstance(im_id, str):\n",
    "                _ = self.out_img_files.index(Path(im_id))\n",
    "                img_file = self.out_img_files[_]\n",
    "                label_file = self.out_label_files[_]\n",
    "            else:\n",
    "                TypeError(\"Invalid image identifier, please input a string to integer\")\n",
    "\n",
    "            img = self.load_image(img_file)\n",
    "            lab = self.load_image(label_file)\n",
    "\n",
    "            return img, lab\n",
    "        else:\n",
    "            raise ValueError(\"No paths for processed image/label files are stored in this class\")\n",
    "\n",
    "\n",
    "    def run(self, clear=False, save=False, contain_lab: bool=False, verbose=False) -> None:\n",
    "        \n",
    "\n",
    "        if clear:\n",
    "            print('Clearing existing output directories')\n",
    "            shutil.rmtree(self.img_out_dir, ignore_errors=True)\n",
    "            shutil.rmtree(self.label_out_dir, ignore_errors=True)\n",
    "            \n",
    "\n",
    "        os.makedirs(self.img_out_dir, exist_ok=True)\n",
    "        os.makedirs(self.label_out_dir, exist_ok=True)\n",
    "        \n",
    "        fnames_h = []\n",
    "        fnames_l = []\n",
    "\n",
    "        # match in_image_files and in_label_files\n",
    "\n",
    "        #TODO: Come up with good way for match_files to handle multiple input directories\n",
    "        self.in_img_paths, self.in_label_paths = self.match_files(self.img_dir, self.label_dir, update=False, paths=True)\n",
    "\n",
    "        aug_params = {\"translation\":[10,10,10]}\n",
    "        patch = [60, 60, 50]\n",
    "        step = [20, 20, 20]\n",
    "\n",
    "        rand_params_gen = self.gen_random_aug(aug_params)\n",
    "\n",
    "        # for each image, label in in_img_files:\n",
    "        out_img_files = []\n",
    "        out_label_files = []\n",
    "\n",
    "        for im_p, lab_p in zip(self.in_img_paths, self.in_label_paths):\n",
    "\n",
    "            # generate a random parameter set\n",
    "            rand_params = next(rand_params_gen)\n",
    "\n",
    "            # Load images\n",
    "            im = self.load_image(im_p)\n",
    "            lab = self.load_image(lab_p)\n",
    "\n",
    "            # apply image augmentations to pairs of images\n",
    "            im, im_suf = self.array_translate(im, rand_params['translation'])\n",
    "            lab, lab_suf = self.array_translate(lab, rand_params['translation'])\n",
    "\n",
    "            # save as patches of size [x,y,z]\n",
    "            \n",
    "            if contain_lab: #Whether to only take patches which contain the label of interest\n",
    "\n",
    "                fname = lab_p.stem\n",
    "                _, b, not_lab = self.img2patches(lab, patch[:], step[:], min_nonzero= 0.1, fname=fname+lab_suf, save=[self.label_out_dir,'.nii'], verbose = False)\n",
    "                out_label_files.extend(b)\n",
    "\n",
    "                fname = im_p.stem\n",
    "                _, a, not_img = self.img2patches(im, patch[:], step[:], fname=fname+im_suf, slice_select=not_lab, save=[self.img_out_dir,'.nii'], verbose = False)\n",
    "                out_img_files.extend(a)\n",
    "\n",
    "            else:\n",
    "                fname = im_p.stem\n",
    "                _, a, not_img = self.img2patches(im, patch[:], step[:], fname=fname+im_suf, save=[self.img_out_dir,'.nii'], verbose = False)\n",
    "                out_img_files.extend(a)\n",
    "\n",
    "                fname = lab_p.stem\n",
    "                _, b, not_lab = self.img2patches(lab, patch[:], step[:], fname=fname+lab_suf, slice_select=not_img, save=[self.label_out_dir,'.nii'], verbose = False)\n",
    "                out_label_files.extend(b)\n",
    "            \n",
    "\n",
    "\n",
    "        # update file locations for use with load_image_pair\n",
    "        self.out_img_files = out_img_files\n",
    "        self.out_label_files = out_label_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U^2-net blocks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int) -> None:\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class InConv(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int) -> None:\n",
    "        super(InConv, self).__init__()\n",
    "        self.conv = DoubleConv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super(Down, self).__init__()\n",
    "        self.mpconv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_ch, out_ch)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.mpconv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, bilinear: bool=True) -> None:\n",
    "        super(Up, self).__init__()\n",
    "\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch // 2, in_ch // 2, 2, stride=2)\n",
    "\n",
    "        self.conv = DoubleConv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:\n",
    "        x1 = self.up(x1)\n",
    "\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, (diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2))\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, in_channels: int, classes: int) -> None:\n",
    "        super(Unet, self).__init__()\n",
    "        self.n_channels = in_channels\n",
    "        self.n_classes =  classes\n",
    "\n",
    "        self.inc = InConv(in_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 512)\n",
    "        self.up1 = Up(1024, 256)\n",
    "        self.up2 = Up(512, 128)\n",
    "        self.up3 = Up(256, 64)\n",
    "        self.up4 = Up(128, 64)\n",
    "        self.outc = OutConv(64, classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.outc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed U^2-Net:\n",
    "Adapter types:\n",
    "\n",
    "`'series_adapter'` = Series Residual Adapter\n",
    "\n",
    "`'parallel_adapter'` = Parallel Residual Adapter\n",
    "\n",
    "`'separable_adapter'` = Residual Adapter which incorporates depthwise separable convolution (what the paper is proposing)\n",
    "\n",
    "Model types:\n",
    "\n",
    "`'universal'` = Model proposed in manuscript that maintains pointwise convoltion parameters across domains while having domain-specific depthwise convolution\n",
    "\n",
    "`'independent'` = Generic U-Net model trained on single image domain\n",
    "\n",
    "`'shared'` = Generic U-Net trained on combinaiton of all image domains(?)\n",
    "\n",
    "- All model types seem to use some sort of [residual adapter](https://www.researchgate.net/figure/Series-vs-parallel-residual-adapters-a-typical-module-of-a-residual-network-inclusive_fig1_324055530) inspired by this [repository](https://github.com/srebuffi/residual_adapters/)\n",
    "\n",
    "\n",
    "Original code key:\n",
    "`nb_tasks` = The number of unique tasks that you are using the model for. So if there are two different image sets you want different models for, then `nb_tasks` = 2\n",
    "\n",
    "`config.module` = the type of adapter you wish to use\n",
    "\n",
    "`config.trainMode` = the model type you wish to use\n",
    "\n",
    "`config.task_idx` = the id number associated with a particular task, this allows for selecting the weights by index for a particular task. So in `q = nn.ModuleList(...)` if you want to use the weights associated with task `i` you use `q[i](input)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U2-Net steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_pool2stride_size(num_pool_per_axis):\n",
    "    '''\n",
    "    Calculate the stride size for the max pooling which occurs, If the num_pool_per_axis[i] for axis 2 is less than the maximum, then\n",
    "    instead of a 2x2x2 pooling it will be a 2x1x2 pooling. With a step size of 1 along the axis, the pooling will not downsample it\n",
    "\n",
    "    \"The number of down-sampling operations per axis is set until the feature map size of the deepest layer reaches as small as 8\"\n",
    "    '''\n",
    "    max_num = max(num_pool_per_axis)\n",
    "    stride_size_per_pool = list()\n",
    "    for i in range(max_num):\n",
    "        unit = [1,2]\n",
    "        stride_size_per_pool.append([unit[i<num_pool_per_axis[0]], unit[i<num_pool_per_axis[1]], unit[i<num_pool_per_axis[2]]])\n",
    "    return stride_size_per_pool\n",
    "\n",
    "\n",
    "class U2Net3D(nn.Module):\n",
    "    def __init__(self, inChans_list: 'List[int]', num_pool_per_axis: 'List[int]', base_outChans: int=16, num_class_list: 'List[int]'=[4],\n",
    "        trainMode: str='universal', module: str='separable_adapter', deep_supervision: bool=True) -> None:\n",
    "        '''\n",
    "        Parameters:\n",
    "        inChans_list: list of input channels for the differnet image types you will be using\n",
    "        num_pool_per_axis: the number of times you wish for this axis to be downsampled by 2x2 max pooling in the endocer portion.\n",
    "                If you have a U-Net with depth 5 but only want to downsample the x-axis 3 times, then you would put [3,5,5]\n",
    "        base_out_Chans: number of output channels for the input image before it goes through the U^2-Net\n",
    "        num_class_list: \n",
    "        depth: the depth of the U-Net\n",
    "        trainMode: The U-Net model type being used\n",
    "        module: The type of adapter you want to use\n",
    "        deep_supervision: whether to apply deep_supervision\n",
    "        '''\n",
    "\n",
    "        super(U2Net3D, self).__init__()\n",
    "        #trainMode = the model type being used\n",
    "        self.trainMode = trainMode\n",
    "        self.module = module\n",
    "        self.deep_supervision = deep_supervision\n",
    "\n",
    "        nb_tasks = len(num_class_list)\n",
    "\n",
    "        self.depth = max(num_pool_per_axis) + 1 # config.num_pool_per_axis firstly defined in train_xxxx.py or main.py\n",
    "        stride_sizes = num_pool2stride_size(num_pool_per_axis)\n",
    "\n",
    "        # Create a module list of the InputTransitions for each input data type\n",
    "        # In other words, if you are running this model on a set of images with 2 channels and a set of\n",
    "        # images with 3 channels, then self.in_tr_list = nn.ModuleList([[2x InputTran],[3x InputTran]])\n",
    "        \n",
    "        # TODO: In this implementaiton, they are applying a 3x3x3 instead of a 1x1x1 like they say in the paper \n",
    "        self.in_tr_list = nn.ModuleList(\n",
    "            [InputTransition(inChans_list[j], base_outChans) for j in range(nb_tasks)]\n",
    "        )\n",
    "\n",
    "        outChans_list = list()\n",
    "        self.down_blocks = nn.ModuleList() #register modules from regular python list.\n",
    "        self.down_samps = nn.ModuleList()\n",
    "        self.down_pads = list() # used to pad as padding='same' in tensorflow\n",
    "\n",
    "        inChans = base_outChans\n",
    "\n",
    "        # Create each level of the encoder and decoder\n",
    "        for i in range(self.depth):\n",
    "            outChans = base_outChans * (2**i)\n",
    "            outChans_list.append(outChans)\n",
    "\n",
    "            # Add another encoder level to the down_block module list\n",
    "            self.down_blocks.append(DownBlock(nb_tasks, inChans, outChans, trainMode, module, kernel_size=3, stride=1, padding=1))\n",
    "\n",
    "            if i != self.depth-1:\n",
    "                # stride for each axis could be 1 or 2, depending on tasks. # to apply padding='SAME' as tensorflow, cal and save pad num to manually pad in forward().\n",
    "                pads = list() # 6 elements for one 3-D volume. originized for last dim backward to first dim, e.g. w,w,h,h,d,d # required for F.pad.\n",
    "                # pad 1 to the right end if s=2 else pad 1 to both ends (s=1). \n",
    "                for j in stride_sizes[i][::-1]:\n",
    "                    if j == 2:\n",
    "                        pads.extend([0,1])\n",
    "                    elif j == 1:\n",
    "                        pads.extend([1,1])\n",
    "                self.down_pads.append(pads) \n",
    "                self.down_samps.append(DownSample(nb_tasks, outChans, outChans*2, trainMode, module, kernel_size=3, stride=tuple(stride_sizes[i]), padding=0))\n",
    "                inChans = outChans*2\n",
    "            else:\n",
    "                inChans = outChans\n",
    "\n",
    "        ## END OF ENCODING\n",
    "        ## BEGINNING OF DECODING\n",
    "\n",
    "        # TODO: Feel like there is a better way to do this...    \n",
    "        self.up_blocks = nn.ModuleList([None] * (self.depth-1))\n",
    "        self.up_samps = nn.ModuleList([None] * (self.depth-1))\n",
    "        self.dSupers = nn.ModuleList()\n",
    "\n",
    "\n",
    "        for i in range(self.depth-2, -1, -1):\n",
    "            #Set up the upsampling which corresponds to the downsampling\n",
    "            self.up_samps[i] = UnetUpsample(nb_tasks, inChans, outChans_list[i], trainMode, module, up_stride=stride_sizes[i])\n",
    "            self.up_blocks[i] = UpBlock(nb_tasks, outChans_list[i]*2, outChans_list[i], trainMode, module, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "            if self.deep_supervision and i < 3 and i > 0:\n",
    "                self.dSupers.append(nn.ModuleList(\n",
    "                    [DeepSupervision(outChans_list[i], num_class_list[j], up_stride=tuple(stride_sizes[i-1])) for j in range(nb_tasks)]\n",
    "                ))\n",
    "\n",
    "            inChans = outChans_list[i]\n",
    "\n",
    "        self.out_tr_list = nn.ModuleList(\n",
    "            [OutputTransition(inChans, num_class_list[j]) for j in range(nb_tasks)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x:torch.Tensor, task_idx: int=0) -> torch.Tensor:\n",
    "        deep_supervision = None\n",
    "\n",
    "        print('Running forward on main class')\n",
    "\n",
    "        out = self.in_tr_list[task_idx](x)\n",
    "\n",
    "        down_list = list()\n",
    "        for i in range(self.depth):\n",
    "            out = self.down_blocks[i](out, task_idx)\n",
    "            # down_list.append(out)\n",
    "            if i != self.depth-1:\n",
    "                down_list.append(out) # will not store the deepest, so as to save memory\n",
    "                # manually padding='SAME' as tensorflow before down sampling\n",
    "                out = F.pad(out,tuple(self.down_pads[i]), mode=\"constant\", value=0)\n",
    "                out = self.down_samps[i](out, task_idx)\n",
    "        \n",
    "\n",
    "        idx = 0\n",
    "        for i in range(self.depth-2, -1, -1):\n",
    "            if self.module in ['parallel_adapter', 'separable_adapter']:\n",
    "                out, share_map, para_map = self.up_samps[i](out, task_idx)\n",
    "            else:\n",
    "                out = self.up_samps[i](out, task_idx)\n",
    "\n",
    "            up_x = out\n",
    "            out = torch.cat((out, down_list[i]), dim=1) #concatenate across channels?\n",
    "            out = self.up_blocks[i](out, task_idx) #TODO: check what up_x used to be, because they also removed it, up_x)\n",
    "\n",
    "            if self.deep_supervision and i < 3 and i > 0:\n",
    "                deep_supervision = self.dSupers[idx][task_idx](out, deep_supervision)\n",
    "                idx += 1\n",
    "        \n",
    "        out = self.out_tr_list[task_idx](out, deep_supervision)\n",
    "\n",
    "        if self.module in ['parallel_adapter', 'separable_adapter']:\n",
    "            return out, share_map, para_map\n",
    "\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    # TODO: DownSampling is not 2x2 MaxPooling, but simply a convolution with a \n",
    "    def __init__(self, nb_tasks, inChans, outChans, trainMode, module, kernel_size=3, stride=1, padding=1) -> None:\n",
    "        super(DownSample, self).__init__()\n",
    "        self.op1 = conv_unit(nb_tasks, inChans, outChans, trainMode, module, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.act1 = norm_act(outChans, meth=\"act\")\n",
    "\n",
    "    def forward(self, x, task_idx: int) -> torch.Tensor:\n",
    "        out = self.op1(x, task_idx)\n",
    "        out = self.act1(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, nb_tasks, inChans, outChans, trainMode, module: str='separable_adapter', residual: bool=True, kernel_size=3, stride=1, padding=1) -> None:\n",
    "        super(DownBlock, self).__init__()\n",
    "        self.module = module\n",
    "        self.residual = residual\n",
    "        self.op1 = conv_unit(nb_tasks, inChans, outChans, trainMode, module, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.act1 = norm_act(outChans, meth=\"act\")\n",
    "        self.op2 = conv_unit(nb_tasks, outChans, outChans, trainMode, module, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.act2 = norm_act(outChans, meth=\"act\")\n",
    "\n",
    "    def forward(self, x, task_idx: int)-> torch.Tensor:\n",
    "\n",
    "        # TODO: These if/else statements are redundant...\n",
    "\n",
    "        if self.module == 'parallel_adapter' or self.module == 'separable_adapter':\n",
    "            out, share_map, para_map = self.op1(x, task_idx)\n",
    "        else:\n",
    "            out = self.op1(x, task_idx)\n",
    "        out = self.act1(out)\n",
    "        if self.module == 'parallel_adapter' or self.module == 'separable_adapter':\n",
    "            out, share_map, para_map = self.op2(out, task_idx)\n",
    "        else:\n",
    "            out = self.op2(out, task_idx)\n",
    "        if self.residual: # same to ResNet\n",
    "            out = self.act2(x + out)\n",
    "        else:\n",
    "            out = self.act2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class UnetUpsample(nn.Module):\n",
    "    def __init__(self, nb_tasks, inChans, outChans,trainMode, module, up_stride=(2,2,2)):\n",
    "        super(UnetUpsample, self).__init__()\n",
    "        self.module = module\n",
    "        self.upsamples = nn.ModuleList(\n",
    "            [Upsample3D(scale_factor=up_stride) for i in range(nb_tasks)]\n",
    "        )\n",
    "        self.op = conv_unit(nb_tasks, inChans, outChans, trainMode, module, kernel_size=3,stride=1, padding=1)\n",
    "        self.act = norm_act(outChans, meth='act')\n",
    "\n",
    "    def forward(self, x, task_idx: int):\n",
    "        out = self.upsamples[task_idx](x)\n",
    "        if self.module == 'parallel_adapter' or self.module == 'separable_adapter':\n",
    "            out, share_map, para_map = self.op(out, task_idx)\n",
    "        else:\n",
    "            out = self.op(out, task_idx)\n",
    "        out = self.act(out)\n",
    "        if self.module == 'parallel_adapter' or self.module == 'separable_adapter':\n",
    "            return out, share_map, para_map\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "def Upsample3D(scale_factor=(2.0,2.0,2.0)):\n",
    "    '''\n",
    "    task specific\n",
    "    '''\n",
    "    # l = tf.keras.layers.UpSampling3D(size=up_strides, data_format=DATA_FORMAT)(l) # by tkuanlun350. # no equavalent in torch?\n",
    "    # scale_factor can also be a tuple. so able to custom scale_factor for each dim.\n",
    "    scale_factor = [float(i) for i in scale_factor] # nn.Upsample wants float values, not integers\n",
    "    upsample = nn.Upsample(scale_factor=tuple(scale_factor), mode='nearest') # ignore the warnings. Only module like upsample can be shown in my visualization. # if using ConvTranspose3d, be careful to how to pad when the down sample method used padding='SAME' strategy.\n",
    "    return upsample\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, nb_tasks, inChans, outChans, trainMode, module: str='separable_adapter', kernel_size=3, stride=1, padding=1):\n",
    "        super(UpBlock, self).__init__()\n",
    "        self.module = module\n",
    "        self.op1 = conv_unit(nb_tasks, inChans, outChans, trainMode, module, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.act1 = norm_act(outChans, meth=\"act\")\n",
    "        self.op2 = conv_unit(nb_tasks, outChans, outChans, trainMode, module, kernel_size=1, stride=1, padding=0)\n",
    "        self.act2 = norm_act(outChans, meth=\"act\")\n",
    "\n",
    "    def forward(self, x, task_idx: int):\n",
    "        if self.module == 'parallel_adapter' or self.module == 'separable_adapter':\n",
    "            out, share_map, para_map = self.op1(x, task_idx)\n",
    "        else:\n",
    "            out = self.op1(x, task_idx)\n",
    "        out = self.act1(out)\n",
    "        if self.module == 'parallel_adapter' or self.module == 'separable_adapter':\n",
    "            out, share_map, para_map = self.op2(out, task_idx)\n",
    "        else:\n",
    "            out = self.op2(out, task_idx)\n",
    "\n",
    "        out = self.act2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class conv_unit(nn.Module):\n",
    "    '''\n",
    "    Conv3D with the addition of adapter, if applicable\n",
    "    '''\n",
    "    def __init__(self, nb_tasks, inChans, outChans, trainMode: str, module, kernel_size: int=3, stride: int=1, padding: int=1) -> None:\n",
    "        super(conv_unit, self).__init__()\n",
    "        self.stride = stride\n",
    "        self.trainMode = trainMode\n",
    "        self.module = module\n",
    "\n",
    "        if self.stride !=1:\n",
    "            self.conv = nn.Conv3d(inChans, outChans, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        elif self.stride == 1:\n",
    "            if trainMode != 'universal': # If not universal, then use generic U-Net setup\n",
    "                self.conv = nn.Conv3d(inChans, outChans, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "            \n",
    "            elif module in ['series_adapter', 'parallel_adapter']:\n",
    "                self.conv = nn.Conv3d(inChans, outChans, kernel_size=kernel_size, stride=stride, padding=padding) # padding != 0 for stride != 2 if doing padding=SAME.\n",
    "                if module == 'series_adapter':\n",
    "                    self.adapOps = nn.ModuleList([conv1x1(outChans) for i in range(nb_tasks)]) # based on https://github.com/srebuffi/residual_adapters/\n",
    "                elif module == 'parallel_adapter':\n",
    "                    self.adapOps = nn.ModuleList([conv1x1(inChans, outChans) for i in range(nb_tasks)]) \n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            elif module == 'separable_adapter':\n",
    "                self.adapOps = nn.ModuleList([dwise(inChans) for i in range(nb_tasks)])\n",
    "                self.pwise = pwise(inChans, outChans)\n",
    "\n",
    "        self.op = nn.ModuleList([norm_act(outChans, meth='norm') for i in range(nb_tasks)])\n",
    "\n",
    "\n",
    "    def forward(self, x, task_idx: int):\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        if self.stride != 1:\n",
    "            out = self.conv(x)\n",
    "            out = self.op[task_idx](out)\n",
    "            return out\n",
    "        elif self.stride == 1:\n",
    "            if self.trainMode != 'universal': # independent, shared\n",
    "                out = self.conv(x)\n",
    "                out = self.op[task_idx](out)\n",
    "            else: #universal\n",
    "                if self.module in ['series_adapter', 'parallel_adapter']:\n",
    "                    out = self.conv(x)\n",
    "                    if self.module == 'series_adapter':\n",
    "                        out = self.adapOps[task_idx](out)\n",
    "                    elif self.module == 'parallel_adapter':\n",
    "                        share_map = out\n",
    "                        para_map = self.adapOps[task_idx](x)\n",
    "                        out = out + para_map\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                    out = self.op[task_idx](out)\n",
    "                    if self.module == 'parallel_adapter':\n",
    "                        return out, share_map, para_map # for visualization of feature maps\n",
    "                    else:\n",
    "                        return out\n",
    "                elif self.module == 'separable_adapter':\n",
    "                    out = self.adapOps[task_idx](x)\n",
    "                    para_map = out\n",
    "                    out = self.pwise(out)\n",
    "                    share_map = out\n",
    "                    out = self.op[task_idx](out)\n",
    "                    return out, share_map, para_map\n",
    "                else:\n",
    "                    raise NotImplementedError(f'Adapter type: {self.module} not implemented')\n",
    "\n",
    "\n",
    "class dwise(nn.Module):\n",
    "    '''Depthwise convolution+normalization'''\n",
    "    def __init__(self, inChans, kernel_size: int=3, stride: int=1, padding: int=1) -> None:\n",
    "        super(dwise, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(inChans, inChans, kernel_size=kernel_size, stride=stride, padding=padding, groups=inChans) #By setting groups=inChans, the kernels are only applied to single channels\n",
    "        self.op1 = norm_act(inChans,meth='both')\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.conv1(x)\n",
    "        out = self.op1(out)\n",
    "        return out\n",
    "\n",
    "class pwise(nn.Module):\n",
    "    '''Pointwise convolution?\n",
    "    '''\n",
    "    def __init__(self, inChans:int, outChans:int, kernel_size: int=1, stride: int=1, padding: int=0) -> None:\n",
    "        super(pwise, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(inChans, outChans, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        out = self.conv1(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class InputTransition(nn.Module):\n",
    "    '''\n",
    "    task specific\n",
    "    '''\n",
    "    def __init__(self, inChans, base_outChans):\n",
    "        super(InputTransition, self).__init__()\n",
    "        self.op1 = nn.Sequential(\n",
    "            nn.Conv3d(inChans, base_outChans, kernel_size=3, stride=1, padding=1),\n",
    "            norm_act(base_outChans)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.op1(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class OutputTransition(nn.Module):\n",
    "    '''\n",
    "    task specific\n",
    "    '''\n",
    "    def __init__(self, inChans, num_class):\n",
    "        super(OutputTransition, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(inChans, num_class, kernel_size=1, stride=1, padding=0)\n",
    "       \n",
    "    def forward(self, x, deep_supervision=None):\n",
    "        out = self.conv1(x)\n",
    "        if deep_supervision is None:\n",
    "            return out\n",
    "        else:\n",
    "            out = torch.add(out, deep_supervision)\n",
    "            return out\n",
    "\n",
    "\n",
    "class conv1x1(nn.Module):\n",
    "    def __init__(self, inChans, outChans=1, stride=1, padding=0):\n",
    "        super(conv1x1, self).__init__()\n",
    "        if self.module == 'series_adapter':\n",
    "            self.op1 = nn.Sequential(\n",
    "                norm_act(inChans,meth='norm'),\n",
    "                nn.Conv3d(inChans, inChans, kernel_size=1, stride=1)\n",
    "                )\n",
    "        elif self.module == 'parallel_adapter':\n",
    "            self.op1 = nn.Conv3d(inChans, outChans, kernel_size=1, stride=stride, padding=padding)\n",
    "        else: #separable adapter\n",
    "            self.op1 = nn.Conv3d(inChans, inChans, kernel_size=1, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.op1(x)\n",
    "        if self.module == 'series_adapter':\n",
    "            out += x\n",
    "        return out\n",
    "\n",
    "def norm_act(nchan, meth='both'):\n",
    "    '''The normalization activation function, or what normalization you are applying to the\n",
    "    data. Either a InstanceNorm3D for meth='norm', a Leaky ReLU for meth = 'act', \n",
    "    or an LeakyReLU(InstanceNorm3D(x)) for meth='both'\n",
    "     '''\n",
    "    norm = nn.InstanceNorm3d(nchan, affine=True) #Calculates the mean and standard deviation across each individual channel for a single example and uses that to normalize values\n",
    "    # act = nn.ReLU() # activation\n",
    "    act = nn.LeakyReLU(negative_slope=1e-2)\n",
    "    if meth=='norm':\n",
    "        return norm\n",
    "    elif meth=='act':\n",
    "        return act\n",
    "    else:\n",
    "        return nn.Sequential(norm, act)\n",
    "\n",
    "class DeepSupervision(nn.Module):\n",
    "    '''\n",
    "    task specific\n",
    "    '''\n",
    "    def __init__(self, inChans, num_class, up_stride=(2,2,2)):\n",
    "        super(DeepSupervision, self).__init__()\n",
    "        self.op1 = nn.Sequential(\n",
    "            nn.Conv3d(inChans, num_class, kernel_size=1, stride=1, padding=0),\n",
    "            norm_act(num_class)\n",
    "        ) \n",
    "        self.op2 = Upsample3D(scale_factor=up_stride)\n",
    "\n",
    "    def forward(self, x, deep_supervision):\n",
    "        if deep_supervision is None:\n",
    "            out = self.op1(x)\n",
    "        else:\n",
    "            out = torch.add(self.op1(x), deep_supervision)\n",
    "        out = self.op2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "qq =  U2Net3D(inChans_list = [1], num_pool_per_axis = [3,3,3], num_class_list=[4], trainMode = 'universal') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "stack size = (784, 60, 60, 50)\n",
      "stack size = (784, 60, 60, 50)\n",
      "stack size = (588, 60, 60, 50)\n",
      "stack size = (588, 60, 60, 50)\n",
      "stack size = (784, 60, 60, 50)\n",
      "stack size = (784, 60, 60, 50)\n",
      "stack size = (784, 60, 60, 50)\n",
      "stack size = (784, 60, 60, 50)\n",
      "stack size = (784, 60, 60, 50)\n",
      "stack size = (784, 60, 60, 50)\n",
      "stack size = (980, 60, 60, 50)\n",
      "stack size = (980, 60, 60, 50)\n",
      "stack size = (588, 60, 60, 50)\n",
      "stack size = (588, 60, 60, 50)\n",
      "stack size = (784, 60, 60, 50)\n",
      "stack size = (784, 60, 60, 50)\n",
      "stack size = (784, 60, 60, 50)\n",
      "stack size = (784, 60, 60, 50)\n",
      "stack size = (784, 60, 60, 50)\n",
      "stack size = (784, 60, 60, 50)\n",
      "stack size = (784, 60, 60, 50)\n",
      "stack size = (784, 60, 60, 50)\n",
      "stack size = (784, 60, 60, 50)\n",
      "stack size = (784, 60, 60, 50)\n",
      "stack size = (784, 60, 60, 50)\n",
      "stack size = (784, 60, 60, 50)\n",
      "stack size = (588, 60, 60, 50)\n",
      "stack size = (588, 60, 60, 50)\n",
      "stack size = (588, 60, 60, 50)\n",
      "stack size = (588, 60, 60, 50)\n",
      "stack size = (588, 60, 60, 50)\n",
      "stack size = (588, 60, 60, 50)\n",
      "stack size = (980, 60, 60, 50)\n",
      "stack size = (980, 60, 60, 50)\n",
      "stack size = (784, 60, 60, 50)\n",
      "stack size = (784, 60, 60, 50)\n",
      "stack size = (784, 60, 60, 50)\n",
      "stack size = (784, 60, 60, 50)\n",
      "stack size = (784, 60, 60, 50)\n",
      "stack size = (784, 60, 60, 50)\n"
     ]
    }
   ],
   "source": [
    "dat = UData('../data/U2Net/Task02_Heart/imagesTr/', '../data/U2Net/Task02_Heart/labelsTr/','../data/U2Net/Task02_Heart/patch_imgTR/','../data/U2Net/Task02_Heart/patch_labTR/')\n",
    "dat.run(clear=True, save=True, contain_lab=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dat.out_label_files) == len(dat.out_img_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, u2_class, label: int) -> None:\n",
    "        self.u2_class = u2_class\n",
    "        self.lab = label\n",
    "\n",
    "    def __len__(self)->int:\n",
    "        return len(self.u2_class.out_img_files)\n",
    "    \n",
    "    def __getitem__(self, index) -> 'tuple[torch.Tensor, torch.Tensor]':\n",
    "        X, Y = self.u2_class.load_image_pair(index)\n",
    "\n",
    "        X = torch.unsqueeze(torch.tensor(X, dtype=torch.float32),0)\n",
    "        Y = torch.unsqueeze(torch.tensor(Y, dtype=torch.float32),0)\n",
    "\n",
    "        return X,Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoader\n",
    "Data loader need to also provide some sort of identification based on the dataset being used if `universal` is the `self.trainMode` value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 5,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 2}\n",
    "\n",
    "training_set = Dataset(dat, 0)\n",
    "training_generator = torch.utils.data.DataLoader(training_set, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 60, 60, 50])\n",
      "Running forward on main class\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): Sizes of tensors must match except in dimension 1. Got 14 and 15 in dimension 2 (The offending index is 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m inp, goal \u001b[39min\u001b[39;00m training_generator:\n\u001b[1;32m      7\u001b[0m     \u001b[39mprint\u001b[39m(inp\u001b[39m.\u001b[39mshape)\n\u001b[0;32m----> 8\u001b[0m     test_output \u001b[39m=\u001b[39m qq(inp,\u001b[39m0\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m     \u001b[39mprint\u001b[39m(test_output\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     10\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.7/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [9], line 133\u001b[0m, in \u001b[0;36mU2Net3D.forward\u001b[0;34m(self, x, task_idx)\u001b[0m\n\u001b[1;32m    130\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup_samps[i](out, task_idx)\n\u001b[1;32m    132\u001b[0m up_x \u001b[39m=\u001b[39m out\n\u001b[0;32m--> 133\u001b[0m out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((out, down_list[i]), dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39m#concatenate across channels?\u001b[39;00m\n\u001b[1;32m    134\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup_blocks[i](out, task_idx) \u001b[39m#TODO: check what up_x used to be, because they also removed it, up_x)\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeep_supervision \u001b[39mand\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m i \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: torch.cat(): Sizes of tensors must match except in dimension 1. Got 14 and 15 in dimension 2 (The offending index is 1)"
     ]
    }
   ],
   "source": [
    "max_epochs = 1\n",
    "\n",
    "for epoch in tqdm(range(max_epochs)):\n",
    "\n",
    "    for inp, goal in training_generator:\n",
    "\n",
    "        print(inp.shape)\n",
    "        test_output = qq(inp,0)\n",
    "        print(test_output.shape)\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit ('3.8.7')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "539b544e2c3fdc58492248d082a132f5e0b4fea63e914fb274c32873997cf2f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
